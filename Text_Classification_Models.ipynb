{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Code : Imports\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import zipfile\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "#Code to Load Tweet Data in a DataFrame\n",
    "datafolder ='Data/'\n",
    "jsonfolder='/JSON/'\n",
    "zippedfilepath = 'JSON.zip'\n",
    "if zippedfilepath:\n",
    "    zippedFolder = zipfile.ZipFile(datafolder + zippedfilepath, 'r')\n",
    "    tweetjsonfiles = zippedFolder.infolist()\n",
    "else:\n",
    "    tweetjsonfiles = os.listdir(datafolder + jsonfolder)\n",
    "\n",
    "tweetsDF = pd.DataFrame()\n",
    "for tweetfile in tweetjsonfiles:\n",
    "    if zippedfilepath:\n",
    "        currjson = json.loads(zippedFolder.open(tweetfile).read())    \n",
    "    else:\n",
    "        currjson = json.loads(open(datafolder + jsonfolder + tweetfile).read())    \n",
    "    currtweetDF = json_normalize(currjson)\n",
    "    tweetsDF = tweetsDF.append(currtweetDF)\n",
    "\n",
    "tweetsDF.index = range(len(tweetsDF.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Sentiment of Each Tweet Based on Number of Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code to set Sentiment Class for each tweet\n",
    "def getClass(fav):\n",
    "    if fav<=4:\n",
    "        return \"NEG\"\n",
    "    elif fav>10:\n",
    "        return \"POS\"\n",
    "    else:\n",
    "        return \"NEU\"\n",
    "tweetsDF['sentiment'] = tweetsDF['favorite_count'].apply(getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features and Labels for Text Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Tweets in English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Code: Filter Languages\n",
    "englishfilter = tweetsDF['lang']=='en'\n",
    "eng_tweets = tweetsDF[englishfilter]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     2,
     18,
     23,
     34
    ]
   },
   "outputs": [],
   "source": [
    "#Helper Functions for Text Feature Extraction\n",
    "\n",
    "def ProcessTextNormal(tweet_text):\n",
    "    tokens = tweet_text.replace('\\n','').split()\n",
    "    txt_features = []\n",
    "    for t in tokens:\n",
    "        tfinal = t.lower()\n",
    "        \n",
    "        #RemoveURLS and Username\n",
    "        if tfinal[0]=='@':\n",
    "            pass\n",
    "        else:\n",
    "            #Handle HTS\n",
    "            if tfinal[0]=='#':\n",
    "                tfinal = tfinal[1:]\n",
    "            txt_features.append(tfinal)\n",
    "    return txt_features\n",
    "\n",
    "def tokenize_tweet(tweet_text):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, preserve_case= False, reduce_len=True)\n",
    "    tweet_tokens = tknzr.tokenize(tweet_text)\n",
    "    #print(tweet_tokens)\n",
    "    temptokens= []  \n",
    "    for t in tweet_tokens:\n",
    "        if len(t)==1 or t[0:5]=='https':\n",
    "            pass\n",
    "        elif t[0]=='#':\n",
    "            tf= t[1:]\n",
    "            temptokens.append(tf)\n",
    "        else:\n",
    "            temptokens.append(t)\n",
    "    return \" \".join(temptokens)\n",
    " \n",
    "    \n",
    "def tokenize_tweets_tweets(tweet_texts):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, preserve_case= False, reduce_len=True)\n",
    "    tweet_tokens = [tknzr.tokenize(tweet_txt) for tweet_txt in tweet_texts]\n",
    "    tweet_tokens_final = []\n",
    "    for tokens in tweet_tokens:\n",
    "        temptokens =[]\n",
    "        for t in tokens:\n",
    "            if len(t)==1 or t[0:5]=='https':\n",
    "                pass\n",
    "            elif t[0]=='#':\n",
    "                tf= \" \"+ t[1:]\n",
    "                temptokens.append(tf)\n",
    "            else:\n",
    "                temptokens.append(t)\n",
    "        tweet_tokens_final.append(temptokens)\n",
    "    return tweet_tokens_final\n",
    "\n",
    "\n",
    "# TestCode\n",
    "# X =candidate_data['full_text'].tolist()[0:1]\n",
    "# X\n",
    "# tokenize_tweets_tweets(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Code : Extract Text Features for POS and Neg Classes\n",
    "tknzr = TweetTokenizer()\n",
    "normal_features = ['full_text','sentiment']\n",
    "candidate_data = eng_tweets[normal_features]\n",
    "classfilter = candidate_data.sentiment.isin(['POS','NEG'])\n",
    "candidate_data = candidate_data[classfilter]\n",
    "candidate_data.index = range(len(candidate_data.index))\n",
    "candidate_data['text_features'] = candidate_data['full_text'].apply(tokenize_tweet)\n",
    "#candidate_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import Classifiers and Helper Librries to Evaluate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report as ClfRep\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "classifiers = {\n",
    "                'MNB' :  {'obj': MultinomialNB(), 'accuracy':0, 'prec':0, 'rec':0, 'fmeasure':0}  ,        \n",
    "                'NN' :  {'obj': MLPClassifier(), 'accuracy':0, 'prec':0, 'rec':0, 'fmeasure':0} ,         \n",
    "                'SGD' :  {'obj': SGDClassifier(loss='hinge', penalty='l2',\\\n",
    "                                 alpha=1e-3, random_state=42, max_iter=5, tol=None),\\\n",
    "                                'accuracy':0, 'prec':0, 'rec':0, 'fmeasure':0}   ,       \n",
    "                'LogReg' :  {'obj': LogisticRegression(random_state=0), 'accuracy':0, 'prec':0, 'rec':0, 'fmeasure':0}  ,        \n",
    "                'SVC' :  {'obj': LinearSVC(), 'accuracy':0, 'prec':0, 'rec':0, 'fmeasure':0}  ,        \n",
    "                 'RF' :  {'obj':  RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), 'accuracy':0, 'prec':0, 'rec':0, 'fmeasure':0}      \n",
    "              \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to Run Experiments\n",
    "def Evaluate(clfIN, X_train, X_test, y_train, y_test,labels, vectorizer):\n",
    "    model_pipeline = Pipeline([\n",
    "                                ('vectorizer', vectorizer),\n",
    "                                ('clf', clfIN)\n",
    "                            ])\n",
    "    trained_model = model_pipeline.fit(X_train.values,y_train)\n",
    "    pred = trained_model.predict(X_test.values)\n",
    "    return accuracy_score(y_test,pred), precision_score(y_test,pred),\\\n",
    "                            recall_score(y_test,pred),f1_score(y_test,pred)\n",
    "\n",
    "def RunExp(classifiers, Xcol, labelsIN, vectorizer):\n",
    "    labels = LabelEncoder()\n",
    "    X =  Xcol\n",
    "    y =  labels.fit_transform(labelsIN)\n",
    "    kfolds = KFold(n_splits=10) \n",
    "    \n",
    "    \n",
    "    \n",
    "    for clfkey,valkey in classifiers.items():\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0 \n",
    "        fmsr = 0\n",
    "        folds = 1\n",
    "        for train_idx, test_idx in kfolds.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            acc, prec, rec, fmeasure = Evaluate(valkey['obj'], X_train, X_test, y_train, y_test,labels, vectorizer)\n",
    "            accuracy += acc\n",
    "            precision += prec\n",
    "            recall += rec\n",
    "            fmsr += fmeasure\n",
    "            folds+=1\n",
    "\n",
    "        valkey['accuracy']= float(accuracy/folds)\n",
    "        valkey['prec']= float(precision/folds)\n",
    "        valkey['rec']= float(recall/folds)\n",
    "        valkey['fmeasure']= float(fmsr/folds)\n",
    "\n",
    "        print([clfkey, valkey['prec'],valkey['rec'],valkey['fmeasure'],valkey['accuracy']])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MNB', 0.39696969696969703, 0.08454078601137426, 0.13024986709197234, 0.7641261691894603]\n",
      "['NN', 0.343939393939394, 0.10192046515575925, 0.14235865144956053, 0.7560414269275029]\n",
      "['SGD', 0.41505312868949235, 0.1337832102537985, 0.18597989163830642, 0.7397834233277272]\n",
      "['LogReg', 0.3939393939393939, 0.07384559884559884, 0.12055167055167054, 0.7607034315895075]\n",
      "['SVC', 0.34632034632034636, 0.0968699601052542, 0.1437721369539551, 0.7468649493965951]\n",
      "['RF', 0.0, 0.0, 0.0, 0.762945914844649]\n"
     ]
    }
   ],
   "source": [
    "RunExp(classifiers, candidate_data['text_features'], candidate_data['sentiment'], CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used different Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MNB', 0.18181818181818182, 0.018668831168831168, 0.03342245989304813, 0.7652769171756514]\n",
      "['NN', 0.3075757575757576, 0.11517804091333503, 0.15621146530237437, 0.7525596766103095]\n",
      "['SGD', 0.45454545454545453, 0.06816378066378066, 0.11358774914924648, 0.7699241686583459]\n",
      "['LogReg', 0.0, 0.0, 0.0, 0.762945914844649]\n",
      "['SVC', 0.36363636363636365, 0.07384559884559884, 0.11919191919191921, 0.7676226726859638]\n",
      "['RF', 0.0, 0.0, 0.0, 0.762945914844649]\n"
     ]
    }
   ],
   "source": [
    "# See Results on TFIDF Vectorizer\n",
    "RunExp(classifiers, candidate_data['text_features'], candidate_data['sentiment'], TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # W2V\n",
    "# import gensim\n",
    "# model = gensim.models.Word2Vec(candidate_data['text_features'].values, size=100)\n",
    "# w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "# # Code taken from http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "# class MeanEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         # if a text is empty we should return a vector of zeros\n",
    "#         # with the same dimensionality as all the other vectors\n",
    "#         self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "#                     or [np.zeros(self.dim)], axis=0)\n",
    "#             for words in X\n",
    "#         ])\n",
    "\n",
    "\n",
    "    \n",
    "# RunExp(classifiers, candidate_data['text_features'], candidate_data['sentiment'], MeanEmbeddingVectorizer(w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def PreprocessWord(word):\n",
    "    pass    \n",
    "\n",
    "#candidate_data['text_features'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pop_features = ['retweet_count', 'favorite_count']\n",
    "#user_features = ['user.verified' ,'user.friends_count','user.followers_count','user.listed_count']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
